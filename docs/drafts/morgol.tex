\documentclass[11pt]{article}
\usepackage{../inc/style}
\title{Morgol specifications}

\newlength{\dlen}
\def\discuss#1{\par\hspace{2em}
\setlength{\dlen}{\textwidth}
\addtolength{\dlen}{-2em}
%\addtolength{\dlen}{-\leftmargin}
\begin{minipage}{\dlen}\footnotesize {\bf\color{red} Discussion:} #1\end{minipage}\par}
\def\say#1#2{\begingroup\par\leftskip1em {\bf #1:} \it #2\par\endgroup}

\begin{document}
% --------------------------------------------------------------------------------------------------------------
\section*{Language syntax and associated semantics}
\discuss{The goal of this external DSL is to be syntactically close from SQL (whereas Scala internal DSL will be more concise), in particular: case insensitivity, comments style and keywords verbosity.}
Let {\tt ident} an identifier string, {\tt longLit}, {\tt doubleLit} and {\tt stringLit} (quoted string) literals and ${\tt type}\in\{{\tt long}, {\tt double}, {\tt string}, {\tt date}\}$ a type declaration. Additional types are converted into these during the parsing.
\discuss{What about user-defined types? As we want to target Scala and C/C++, we should limit ourselves to case classes/C struct, but these could easily be unfolded into multiple values maintained in several columns. Not clear if there is any benefit to allow arbitrary types.}
For conciseness, we denote: \verb$rs(a,b) := (a (b a)*)?$ and \verb$rs1(a,b) := a (b a)*$. Comments are delimited by: {\tt ----...\textbackslash n} , {\tt //...\textbackslash n} or {\tt /*...*/}.\\
The system declaration file format is defined as:
\begin{verbatim}
system ::= source* mapdef* trigger*
\end{verbatim}

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Sources}
\begin{verbatim}
source ::= "CREATE" ("STREAM" | "TABLE") schema "FROM" input split adaptor ";"
schema ::= name "(" rs1(field type, ",") ")"
input  ::= "FILE" quotedPath
split  ::= ("LINE" | quotedSeparator) "DELIMITED"
         | "FIXEDWIDTH" sizeInBytes
         | "PREFIX" headerBytes
adaptor ::= name "(" rs(option ":=" value, ",") ")"
\end{verbatim}

\discuss{Andres suggests that input should only be defined by an inteface that the programmer implements (we provide some built-ins: file, network, ...) to support arbitrary sources. Flexibility is important to accomodate with existing streams. One specialized stream would be a clock (see clock join in \hyperref[pb:pagerank]{PageRank}).
\say{Thierry}{While I agree with Andres, is it most beneficial to have this also in the external DSL or only available in Scala internal DSL?}
}
\discuss{Do we want to have the converse (output streams)? Are they handled in user-defined triggers, with an interface or through user-defined (Scala) functions (leveraging the apply expression)?}

Source is either input stream (content varies while the program is running and generate events) or fixed tables (constant relations). The implemented adaptors and their respective options are:\ul
\item {\tt ORDERBOOK}: finance events in CSV, with the columns: {\tt transaction\_id}:int, {\tt broker\_id}:int, {\tt evt\_type}:string$\in\{B,S,E,D\}$ (place bid, place ask, match, cancelled), {\tt volume}:int, {\tt price}:double.\ul
	\item {\tt brokers}: number of brokers (integer)
	\item {\tt bids}, {\tt asks}: stream names as stringLit
	\item {\tt deterministic}: boolean defining whether broker id is assigned randomly
	\ule
\item {\tt CSV}\ul
	\item {\tt name}: string, schema name
	\item {\tt schema}: stringLit of comma-separated types
	\item {\tt delimiter}: column delimiter as string, by default a comma
	\item {\tt action} $\in\{\text{insert, delete, both}\}$. If both, first column denotes the transaction\_id, the second is $\in\{0,1\}$ and is 1 if it is an insertion, following columns are the tuple.
	\ule
\ule

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Expressions}
\begin{verbatim}
expr   ::= "(" expr ")" | ...
mapref ::= name "[" rs(key, ",") "]"
add    ::= expr "+" expr
mul    ::= expr "*" expr
aggsum ::= "SUM" "(" ("[" rs(key, ",") "]" ",")? expr ")"
exists ::= "EXISTS" "(" expr ")" | "{" expr "}"
lift   ::= "(" ident "^=" expr ")"
cmp    ::= expr ("="|"=="|"!="|"<>"|">"|"<"|">="|"<=") expr
apply  ::= ident "(" rs(expr, ",") ")" // type inferred from library
ref    ::= ident
const  ::= longLit | doubleLit | stringLit
\end{verbatim}
Minor changes simplify the M3 language, otherwise semantics remains identical.
Table access are replaced by map references because they share the same semantics. 
We have aliases for exists (conciseness) and two comparison operators (compatibility).
AggSum with no key is equivalent to empty key.
We need to lookup built-in functions return type (and possibly accepted arguments).
\discuss{For function lookup, how to avoid declaration duplication (Bytecode inspection)?}
\discuss{Simplify grammar by also using {\tt =} for lift? This introduce ambiguity between lift and cmp as they are differentiated only by the context (depending if the lhs ref is bound), but after all, comparison of two bound variable is semantically equivalent to lift one as the other.}

\discuss{Do we need explicit AggMin and AggMax operations or are these handled by maps?}
% aggmin ::= "Min" "(" ("[" rs(key, ",") "]" ",")? expr ")"
% aggmax ::= "Max" "(" ("[" rs(key, ",") "]" ",")? expr ")"

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Maps (internal structures and queries)}
A table has an associated map in which the table will be loaded. A table does not generate trigger whereas there are always triggers associated with a stream. The default type of a map is {\tt long} if not specified. A map associated with an input stream is implicitly created if the stream is accessed by a mapref (lookup/foreach). The output keywords has exactly the same semantics as declaring the map being result of a query in M3.
% In Andres original proposal it was PUBLIC instead of OUTPUT. Good idea :)

\begin{verbatim}
mapdef ::= "DECLARE" "OUTPUT"? "MAP" name "[" rs(key, ",") "]" ("[" rs(aggkey, ",") "]")? 
                                 (":" type)? (":=" expr)? mapopt? ";" 
key    ::= ident ":" type
aggkey ::= "filter"? ("min"|"max"|"sum") "(" ident ")" ":" type
mapopt ::= "WITH" "(" rs(option ":=" value, ",") ")"
\end{verbatim}
\discuss{Are all type annotations necessary?
	\say{Andres}{Aggregation keys (and possibly keys) types should be inferred}
	\say{Thierry}{Agree, but not sure we can always derive the correct type because we might have ordering issues and it gets harder with recursion. I would prefer forcing them now and make them optional later. Also note that the aggregation does not need to happen on a key but any bound variable in rhs.}}

If the expression is not defined, the map is user-managed and the programmer needs to write his own triggers to maintain it.
\discuss{Are user-defined triggers really need? Have they more flexibility/power than expressions?}

{\bf Semantics}
\discuss{most suble topic as we want to have a simple syntax for 3 concerns:\\
-- Number of shortest paths between two vertices (similar as \cite{socialite} running example)\\
-- Simple grouping with aggregations as in TPC-H query 1\\
-- Range conditions as in AXFinder financial query\\
and blend this nicely with incremental domain maintenance in the implementation.}

First key part is the grouping columns, second part is aggregation values for the group, multiplicity is the number of tuples in the group. Aggregation values semantics is <<element>> for write and <<aggregate>> for read. Finally, if a <<filter>> keyword precedes the aggregation key, tuples that do not have the same aggregation value will be discarded.

The equivalent SQL query of a filtered aggregation key $k$ is a nested query:
\begin{lstlisting}[language=sql]
SELECT keys AS keys_out, aggrs FROM map
WHERE k = (SELECT agg_k(k) FROM map WHERE keys=keys_out)
GROUP BY keys
\end{lstlisting}
\discuss{Do we maintain tuples that have been filtered out ? If yes, how does it combine with incremental domain maintenance? How do we distinguish tuples that we need to re-compute? How to maintain correct multiplicity?
\say{Milos}{disagree with this semantics, think that the resulting multiplicity of aggregation should be 1 to keep coherency with the SQL.}
\say{Thiery}{SQL multiplicity of 1 is only for displaying purposes. We retain more power by having the number of elements aggregated to compute average and count easily afterward.}
}

{\bf Map options:}\\
The options that can be declared for a map are the location and the value threshold:
\ul\item The location is defined by one or multiple tags (logical group name) for the map. Nodes join the cluster with one or more tags: they accept to be in charge of a partition of all maps attributed to the tag.
	Moreover, we can tune manually the locality with hashing function.
\ule
\discuss{What if the number of nodes is variable, in particular what if nodes are leaving/joining  (failure/recovery) the system? There are two approaches: static hashing with redundancy (that is read/write from multiple nodes or \href{http://lpd.epfl.ch/alistarh/DistAlgoFall08/da08-GroupMembershipVSC.pdf}{group membership}. Is there an existing framework we can build on?}
\ul\item The threshold is the relative difference in keys/values for a tuple to be considered as an update of the map for recursion. This only applies to double types.
\ule
\discuss{Not clear to which field the threshold relates. Do we need other options ?}
\ul\item Andres suggests that we should have options to hint the compiler about which structure we want to maintain secondary indices (tree, hash) in; to address inequality joins for example. When automatic inference  detects sufficiently well these cases, we can remove this option.
\ule

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Triggers (user-defined)}
\begin{verbatim}
trigger ::= "ON" ("+"|"-") streamName "(" ident ("," ident)* ")" "{" (stmt)* "}"
          | "ON" "SYSTEM" "READY" "{" (stmt)* "}"
stmt    ::= mapref (":" "(" expr ")")? ("+="|":=") expr ";"
          | "CALL" ("+"|"-") streamName "(" rs(expr, ",") ")"
\end{verbatim}
The triggers being defined by the user are called before any other trigger. It is not possible to override an automatically generated trigger.
\discuss{Do we want to have an extra syntax for user-defined triggers?}
\discuss{When do we call user-defined triggers (before all automatic triggers)?}

% --------------------------------------------------------------------------------------------------------------
\section*{Examples}
\subsubsection*{Problem 1: betweenness centrality}
Let our stream be $edge(x,y,l)$ (with implicit map). To compute the \href{http://en.wikipedia.org/wiki/Betweenness_centrality}{betweenness centrality} $g(v)$ of graph nodes $v$, we apply the following rules:
\[\begin{array}{rcl}
edge[x,y,l] &:=& \text{\it implicitly, corresponding to input stream} \\
spath[x,y][{\rm filter} \min(l)] &:=& edge[x,y,l] +  edge[x,z,l_1] \times spath[z,y,l_2] \times (l\hat= l_1+l_2)\\
spath_v[x,y,v,l] &:=& spath[x,v,l_1] \times spath[v,y,l_2] \times (l\hat= l_1+l_2)\\
centrality[v] &:=& \exists(spath[x,y,l]) \times (spath_v[x,y,v,l] / spath[x,y,l])
\end{array}\]

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Problem 2: PageRank} \label{pb:pagerank}
The iterative method of \href{http://en.wikipedia.org/wiki/PageRank#Iterative}{PageRank} is defined for $N$ pages with a matrix $M=(K^{-1} A)^T$ where $A$ is the adjacency matrix of the graph and $K$ the diagonal matrix with out-degrees in diagonal. We synchronously compute (usually $d=0.85$):
\[R(t+1)=d\cdot M\cdot R(t)+\dfrac{1-d}{N}{\bf 1} \qquad \text{with {\bf 1} a column vector of 1s and }R(0)={\bf 1}\cdot \frac{1}{n}\]

Computing this formula asynchronously does not affect correctness but convergence speed (as computation wavefront might visit loops more frequently than necessary).

\[\begin{array}{rrl}
edge[x,y] &:=& \text{\it input stream, adjacency matrix} \\
node[x] &:=& \text{\it input stream} \in\{0,1\} \text{ or }\exists(edge[x,y] + edge[z,x]) \\
out[x] &:=& {\rm AggSum}([],edge[x,y]) \\
N &:=& {\rm AggSum}([],node[x]) \\
weight[x] &:=& node[x] \times {\rm AggSum}([], \dfrac{edge[y,x] \times weight[y]}{out[y]}) \times d + \dfrac{1-d}{N}\\
&& \text{with }(threshold := 10^{-5})
\end{array}\]

\discuss{To execute statements synchronously, we might have multiple options:\ul
\item {\bf Clock join:} associate a timestamp attribute with all map and create a clock stream. When a clock tick is inserted, the next iteration is computed (join $data \times tick$); when the tick is removed, the associated data can be removed from the map. Issues: deletion trigger of weight must be \underline{overridden} not to be recursive. How do we check that all elements of a tick have been computed ?
\say{Andres}{The nice thing with a custom clock stream is that you can tick based on the number of iterations, a timeout or any other event.}
\item \textbf{Limited iterations:} To execute statements synchronously, we could compute only a fixed number of steps. We can then make that number of step vary to reach threshold requirements.
\ule}

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Problem 3: Optimization for existing queries}
The \href{http://www.tpc.org/tpch/spec/tpch2.16.0.pdf}{TPC-H} query 1 is a simple aggregation over one table.
\begin{lstlisting}[language=sql]
SELECT returnflag, linestatus, 
  SUM(quantity) AS sum_qty,
  SUM(extendedprice) AS sum_base_price,
  SUM(extendedprice * (1-discount)) AS sum_disc_price,
  SUM(extendedprice * (1-discount)*(1+tax)) AS sum_charge,
  AVG(quantity) AS avg_qty,
  AVG(extendedprice) AS avg_price,
  AVG(discount) AS avg_disc,
  COUNT(*) AS count_order
FROM lineitem WHERE shipdate <= DATE('1997-09-01')
GROUP BY returnflag, linestatus;
\end{lstlisting}
If we can detect that only aggregation is done over {\it lineitem}, (formally $\Delta Q1a=f(\Delta lineitem)$), we can eliminate the associated map. The query can be rewritten as:
\[\begin{array}{l}
\text{CREATE STREAM }lineitem(quantity,extendedprice,discount,tax,shipdate)\text{ FROM }...\vspace{4pt}\\
\text{DEFINE MAP }Q1a[r,s][sum(q),sum(ep),sum(dp),sum(c),sum(d)] := lineitem[q,ep,d,t,sd] \times\\
\qquad (dp \hat= ep\times(1-d)) \times (c \hat= ep\times(1-d)\times(1+t)) \times (sd < \text{date("1997-09-01")})\vspace{4pt}\\
\text{DEFINE MAP }Q1[r,s,sq,sep,sdp,sc,aq,aep,ad,n] := (n \hat= Q1[r,s][sq,sep,sdp,sc,sd]) \times\\
\qquad (aq \hat= sq/n) \times (aep \hat= sep/n) \times (ad \hat= sd/n)\vspace{4pt}\\
\text{DEFINE QUERY }QUERY1 := Q1
\end{array}\]

\discuss{How can we implement efficiently range condition for AXFinder}
%\begin{lstlisting}[language=sql]
%SELECT b.broker_id, SUM(a.volume + (-1 * b.volume)) AS axfinder
%FROM   bids b, asks a
%WHERE  b.broker_id = a.broker_id
%  AND  ( (a.price + ((-1) * b.price) > 1000) OR
%         (b.price + ((-1) * a.price) > 1000) )
%GROUP BY b.broker_id;
%\end{lstlisting}

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Problem 4: Hubs and authorities}
The \href{http://en.wikipedia.org/wiki/HITS_algorithm}{HITS algorithm} is a precursor of the PageRank algorithm.

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Problem 5: Mutual neighbors}

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Problem 6: Connected components}
\href{http://en.wikipedia.org/wiki/Connected_component_(graph_theory)}{Connected components} probably implemented with \href{http://en.wikipedia.org/wiki/Disjoint-set_data_structure}{disjoint sets}.

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Problem 7: Triangles}

% --------------------------------------------------------------------------------------------------------------
\subsubsection*{Problem 8: Clustering coefficients}

\discuss{Can we do more, something that DataLog could not do ? Find other interesting algorithms.}


\begin{verbatim}
%- I would suggest to get rid of ON SYSTEM READY and have a stream of system events instead
%==> reduce tables to only what necessary ? not sure we can always do that.

-> MaxFlow Min Cut
-> Hubs and authorities
-> what's the timer usage <- clock as custom stream
-> bloom query with locality example
\end{verbatim}

{\center --------------------------------------- legacy discussion ---------------------------------------}
\begin{verbatim}
mapdef ::= "DECLARE" "MAP" name "[" rs(key, ",") "]" ("[" rs(aggkey, ",") "]")? 
                                 (":" type)? ":=" expr mapopt? ";" 
key    ::= ident ":" type
aggkey ::= ("min"|"max"|"sum") "(" ident ")" ":" type
mapopt ::= "WITH" "(" rs(option ":=" value, ",") ")"
\end{verbatim}

{\color{red}
XXX: milos' issue: if we have only multiplicity 1 we can put aggr in multiplicity\\
XXX: mark for inequality comparison internally ? (maintain in tree)
}

\discuss{It is not very clear whether aggregation keys should be separated from other keys as they must be provided for addition and deletion, but are read differently.}
For recursion, we consider that a tuple has changed if it is addition creates a change to the map being read. This can be either: an insertion/deletion, a modification in the value (multiplicity), or a change of the aggregated values.
\discuss{We need to detect these "modifications" efficiently and compute its delta over the map to create a reaction to it (call other triggers with updates values). It is not clear how a change in an aggregate value will be treated. Should it be a "deletion/insertion" (not desirable because if we have deletion, we have endless loop in shortest path) or a "value update"?
	\say{Thierry}{I think there is many issues in the nitty gritty details of the implementation that we didn't covered so far.}
We still do not achieve the effect that is claimed in SociaLite: having one single map to compute the shortest path \textit{and} computing its multiplicity. What should the return value of a map containing aggregations?
	\say{Thierry}{The returned value should be the multiplicity of the min/max/sum so that we can achieve average and counting easily. We can easily modify it by wrapping into an exists. The shortest path iteration should be done with the number of added tuples.}
	\say{Milos}{Returned value should be 1 when there is aggregted values (1 aggregate tuple). The number of shortest path is computed using two different maps, one for the minimum length, one for the count.}
}
How do these concerns blend with recursion and incremental maintenance?

{\center --------------------------------------- legacy discussion end ---------------------------------------}

% ------------ LEGACY PROBLEMS --------------
%\subsubsection*{Problem 1: betweenness centrality}
%Let our fact table be $edge(x,y)$ we want to compute the \href{http://en.wikipedia.org/wiki/Betweenness_centrality}{betweenness centrality} $g(v)$ of graph nodes $v$, we apply the following rules:
%\[\begin{array}{rcl}
%edge[x,y] &:=& \text{\it implicitly defined and related to corresponding input stream} \\
%path[x,y,len] &:=& \exists(edge[x,y]) \times (len\hat= 1) +  \exists(edge[x,z]) \times \exists(path[z,y,l]) \times (len\hat= 1+l) \times (x\ne y)\\
%path_v[x,y,v,len] &:=& \exists(path[x,v,l_1]) \times \exists(path[v,y,l_2]) \times (len\hat= l_1+l_2) \times (x\ne y) \\
%centrality[v] &:=& \exists(path[x,y,m]) \times (\exists(path[x,y,s] \times (s<m)) = 0) \times \left(\dfrac{{\rm Sum}(path_v[x,y,v,m])}{path[x,y,m]}\right)
%\end{array}\]
%
%By adding the condition $(x\ne y )$ in the $path$ definition, we prevent cycles. <<< WRONG
%
%\subsubsection*{Problem 2: PageRank}
%The iterative method of \href{http://en.wikipedia.org/wiki/PageRank#Iterative}{PageRank} is defined for $N$ pages with a matrix $M=(K^{-1} A)^T$ where $A$ is the adjacency matrix of the graph, $K$ the diagonal matrix with out-degrees in diagonal. We compute \[R(t+1)=d\cdot M\cdot R(t)+\dfrac{1-d}{N}{\bf 1} \qquad \text{with {\bf 1} a column vector of 1 and }R(0)={\bf 1}\cdot \frac{1}{n}\]
%The issues with the matrix model is that all computation are done synchronously.
%
%\textbf{Naive version:} To avoid explicit synchronization, we have 2 solutions: maintaining versions in the database or normalizing $|R|=1$ after every computation.
%\[\begin{array}{rrl}
%node[x] &:=& \text{\it input stream} \in\{0,1\} \\
%edge[x,y] &:=& \text{\it input stream, adjacency matrix} \\
%weight[x] &:=& node[x] \times (w_0\hat=weight[x]) \times \left( w_0 + (w_1\hat=nw) \times \left(
%\dfrac{|w_1 - w_0|}{w_1+w_0} > {\rm threshold}\right) \times (w_1-w_0) \right) \\ \\
%&& \text{where } nw= {\rm AggSum}([], \dfrac{edge[y,x] \times weight[y]}{{\rm AggSum}([],edge[y,z])}) \times d + \dfrac{1-d}{{\rm AggSum}([],node[v])}\\
%\end{array}\]
%The shortcomings with this approach are the non-preservation of global weight and injection of initial coefficient. The $d$ weighting should mitigates this issue (but not sure at all !!). We also must ensure that we don't call recursively if the value did not change (encode this with threshold in map declaration or in another function?).
%
%\textbf{Clock join:} One possible idea to virtually execute the program synchronously is to have a clock stream and a timestamp attribute for all maps. When a clock tick is inserted, the next iteration can be computed (join $data \times tick$); when the tick is removed, the associated data can be removed from the map. Issues:\ul
%\item Deletion trigger of weight must be \underline{overridden} to only remove data (and avoid recursion).
%\item How to know that all the data at a tick has been fully computed ?
%\ule
%\[\begin{array}{rrl}
%tick[t] &:=& \text{\it logical clock} \in \{0,1\} \\
%node[x] &:=& \text{\it input stream} \in\{0,1\} \\
%edge[x,y] &:=& \text{\it input stream} \\
%weight[x,t] &:=& node[x] \ \times {\rm AggSum}([x], \dfrac{edge[y,x] \times weight[y,t-1]}{{\rm AggSum}([],edge(y,z))}) \times d + \dfrac{1-d}{{\rm AggSum}([],node[z])}
%\end{array}\]
% ------------ LEGACY PROBLEMS END --------------

\section*{Roadmap}
Goals:\ul
\item Find use cases and examples
\item Design the language, allow incrementalization by reusing most of existing infrastructure, and combine the power of Datalog and triggers.
\item Get a clean and simple semantics that can be used and understood easily in various domains: windowed streams, state machine, continuous queries.
\item Define distribution strategy
\item Data locality: optional location specification. Must be orthogonal to other concerns (tag with @all, @group, @hashed ?)
\item Simulate timestamps / iteration numbers
\ule

Integrate features from
Socialite\cite{socialite},
WebDamLog\cite{webdamlog},
Bloom lattices\cite{bloom_lattices},
Dedalus\cite{dedalus},
CALM\cite{bloom_calm},
Pregel\cite{pregel},
DatalogFS\cite{datalog_fs} in DBToaster\cite{dbtoaster09,dbtoaster11}.

--------------------------------------------------------------

{\color{red}
XXX: for non-stratified programs, provide the user facilities like $<+, <-$ to do operations at the next step.
XXX: we need to be able to define aggregation in one of the keys
XXX: have column-oriented / nested tables to avoid keys duplication? how is this compatible with secondary indices ?}
connect map to its delta to use them
recursion = non-recursive with fixed \# iterations. orthogonal serie of maps
\begin{verbatim}
Goal:
=> inflationary semantics, what's cleanest language ?
look at orchestra (datalog + incrementally) [not very interesting]
- explicit deltas, use cases, triggers, active dbs
- what make it easier to do incrementalization easier
\end{verbatim}

--------------------------------------------------------------

% Bibliography
\bibliographystyle{plain} % acm
\def\pdfurl#1{\href{#1}{\footnotesize pdf}}
{\small \bibliography{../inc/bibliography.bib}}
\end{document}